{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Retrieval-based Customer Support Agent (FAQ + Tickets)\n",
        "\n",
        "This notebook builds an end-to-end retrieval pipeline for a customer support use case:\n",
        "- Generate / load a dataset of FAQs + past support tickets\n",
        "- Clean and chunk text\n",
        "- Embed text with **Euri** embedding API (example code included; set your API key)\n",
        "- Store vectors in FAISS (local) or Qdrant (optional example)\n",
        "- Build a simple retrieval-based chatbot that returns suggested answers\n",
        "\n",
        "⚠️ **Notes**:\n",
        "- This notebook contains example code that calls the *Euri* embedding endpoint using `requests`. Replace the placeholder URL and API key with your real values.\n",
        "- Install dependencies before running the cells (instructions included). When running locally in VS Code, run the notebook with a Python interpreter that has required packages installed.\n",
        "- The notebook is intended to be runnable as-is after installing dependencies. No remote execution is done by this file itself.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e9569c4c",
      "metadata": {},
      "source": [
        "## 1) Requirements / Install\n",
        "\n",
        "Run these (once) in your environment / terminal before executing the notebook cells:\n",
        "```bash\n",
        "pip install pandas numpy scikit-learn faiss-cpu requests qdrant-client sentence-transformers tqdm\n",
        "```\n",
        "If you prefer Qdrant instead of FAISS, install and configure Qdrant server and set `USE_QDRANT=True` in the notebook.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "88a18e7b",
      "metadata": {},
      "source": [
        "## 2) Helper pipeline functions\n",
        "\n",
        "Below are utility functions used in the pipeline: cleaning, chunking, embedding (Euri example), and FAISS index creation.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "f33ff1b1",
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "import re\n",
        "import json\n",
        "import math\n",
        "from typing import List, Dict, Tuple\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from tqdm import tqdm\n",
        "\n",
        "def clean_text(text: str) -> str:\n",
        "    \"\"\"Simple cleaning: remove extra whitespace, normalize unicode, strip HTML tags if present.\"\"\"\n",
        "    if not isinstance(text, str):\n",
        "        return \"\"\n",
        "    text = text.replace('\\n', ' ').replace('\\r', ' ')\n",
        "    text = re.sub(r'<[^>]+>', ' ', text)  # remove simple HTML\n",
        "    text = re.sub(r'\\s+', ' ', text).strip()\n",
        "    return text\n",
        "\n",
        "def chunk_text(text: str, max_chars: int = 500) -> List[str]:\n",
        "    \"\"\"Chunk text into pieces of up to max_chars characters, trying to split on sentence boundaries.\"\"\"\n",
        "    text = text.strip()\n",
        "    if len(text) <= max_chars:\n",
        "        return [text]\n",
        "    sentences = re.split(r'(?<=[.!?])\\s+', text)\n",
        "    chunks = []\n",
        "    current = []\n",
        "    cur_len = 0\n",
        "    for s in sentences:\n",
        "        if cur_len + len(s) + 1 <= max_chars:\n",
        "            current.append(s)\n",
        "            cur_len += len(s) + 1\n",
        "        else:\n",
        "            if current:\n",
        "                chunks.append(' '.join(current).strip())\n",
        "            # if sentence itself longer than max_chars, hard-split\n",
        "            if len(s) > max_chars:\n",
        "                for i in range(0, len(s), max_chars):\n",
        "                    chunks.append(s[i:i+max_chars])\n",
        "                current = []\n",
        "                cur_len = 0\n",
        "            else:\n",
        "                current = [s]\n",
        "                cur_len = len(s) + 1\n",
        "    if current:\n",
        "        chunks.append(' '.join(current).strip())\n",
        "    return chunks\n",
        "\n",
        "def flatten_chunks(records: List[Dict], max_chars=500) -> List[Dict]:\n",
        "    \"\"\"Take records (each with 'id','text','meta') and produce chunked records with chunk_id and chunk_text.\"\"\"\n",
        "    out = []\n",
        "    for r in records:\n",
        "        text = clean_text(r.get('text',''))\n",
        "        chunks = chunk_text(text, max_chars=max_chars)\n",
        "        for i, c in enumerate(chunks):\n",
        "            out.append({\n",
        "                'orig_id': r.get('id'),\n",
        "                'chunk_id': f\"{r.get('id')}_c{i}\",\n",
        "                'text': c,\n",
        "                'meta': r.get('meta', {})\n",
        "            })\n",
        "    return out\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "13c80e34",
      "metadata": {},
      "source": [
        "## 3) Euri embedding example (replace with your real endpoint & key)\n",
        "\n",
        "This cell shows how to call Euri's embedding endpoint. Update `EURI_API_KEY` and `EURI_EMBED_URL`.\n",
        "\n",
        "The function `get_embeddings_euri` accepts a list of strings and returns a numpy array of vectors.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "32612c77",
      "metadata": {},
      "outputs": [],
      "source": [
        "import requests\n",
        "\n",
        "EURI_API_KEY = os.environ.get('EURI_API_KEY', '{EURi_API_KEY}')\n",
        "EURI_EMBED_URL = os.environ.get('EURI_EMBED_URL', 'https://api.euri.example/v1/embeddings')\n",
        "\n",
        "def get_embeddings_euri(texts: List[str], batch_size: int = 16) -> np.ndarray:\n",
        "    \"\"\"Call Euri embedding API and return embeddings as a numpy array.\n",
        "    **Replace** EURI_EMBED_URL with the actual endpoint and ensure EURI_API_KEY is set in your environment.\n",
        "    ```\n",
        "    Example request payload (depends on Euri API; adjust as needed):\n",
        "    POST /v1/embeddings\n",
        "    {\n",
        "      \"model\": \"euri-embed-1\",\n",
        "      \"input\": [\"text1\", \"text2\"]\n",
        "    }\n",
        "    ```\n",
        "    \"\"\"\n",
        "    headers = {\n",
        "        'Authorization': f'Bearer {EURI_API_KEY}',\n",
        "        'Content-Type': 'application/json',\n",
        "    }\n",
        "    embeddings = []\n",
        "    \n",
        "    for i in range(0, len(texts), batch_size):\n",
        "        batch = texts[i:i+batch_size]\n",
        "        payload = {\n",
        "            'model': 'euri-embed-1',\n",
        "            'input': batch\n",
        "        }\n",
        "        resp = requests.post(EURI_EMBED_URL, headers=headers, json=payload)\n",
        "        if resp.status_code != 200:\n",
        "            raise RuntimeError(f'Euri embedding API error {resp.status_code}: {resp.text}')\n",
        "        data = resp.json()\n",
        "        # Adjust depending on Euri response format. Common: {\"data\": [{\"embedding\": [...]}, ...]}\n",
        "        for item in data.get('data', []):\n",
        "            embeddings.append(item['embedding'])\n",
        "    return np.array(embeddings, dtype=np.float32)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f803544c",
      "metadata": {},
      "source": [
        "## 4) FAISS index creation & search\n",
        "\n",
        "The example below creates a FAISS index (IndexFlatIP) using inner product similarity. We normalize vectors to use cosine similarity.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "59177c46",
      "metadata": {},
      "outputs": [],
      "source": [
        "def build_faiss_index(embeddings: np.ndarray):\n",
        "    try:\n",
        "        import faiss\n",
        "    except Exception as e:\n",
        "        raise RuntimeError('faiss is required. pip install faiss-cpu') from e\n",
        "    # Normalize for cosine similarity\n",
        "    norms = np.linalg.norm(embeddings, axis=1, keepdims=True)\n",
        "    norms[norms==0] = 1e-6\n",
        "    emb_norm = embeddings / norms\n",
        "    dim = emb_norm.shape[1]\n",
        "    index = faiss.IndexFlatIP(dim)\n",
        "    index.add(emb_norm.astype(np.float32))\n",
        "    return index\n",
        "\n",
        "def search_faiss(index, query_emb: np.ndarray, top_k: int = 5) -> Tuple[np.ndarray, np.ndarray]:\n",
        "    # normalize\n",
        "    qn = query_emb / (np.linalg.norm(query_emb) + 1e-10)\n",
        "    D, I = index.search(qn.reshape(1, -1).astype(np.float32), top_k)\n",
        "    return D[0], I[0]\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a0c6c90d",
      "metadata": {},
      "source": [
        "## 5) Example dataset generation (FAQs + support tickets)\n",
        "Creates a small example CSV `support_data.csv` so you can run the pipeline immediately. Replace with your real dataset (CSV/JSON) as needed.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "id": "5e37aa4b",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Wrote enriched support CSV to ./support_data.csv\n"
          ]
        }
      ],
      "source": [
        "SUPPORT_CSV = './support_data.csv' \n",
        "\n",
        "def generate_example_dataset(path=SUPPORT_CSV):\n",
        "    rows = []\n",
        "    # FAQs\n",
        "    rows.append({'id': 'faq_1', 'type': 'faq', 'subject': 'How to reset my password?', \n",
        "                 'text': \"To reset your password go to Settings -> Account -> Reset password. You'll receive an email with a reset link.\", \n",
        "                 'answer': \"Go to Settings → Account → Reset password. Check your email and follow the link.\"})\n",
        "    \n",
        "    rows.append({'id': 'faq_2', 'type': 'faq', 'subject': 'How to cancel subscription?', \n",
        "                 'text': \"You can cancel from the Billing page in your account settings or contact support.\", \n",
        "                 'answer': \"Open Billing → Cancel subscription. Contact support if you have trouble.\"})\n",
        "    \n",
        "    rows.append({'id': 'faq_3', 'type': 'faq', 'subject': 'How to update billing information?', \n",
        "                 'text': \"To update your billing info, go to Account -> Billing -> Update payment method.\", \n",
        "                 'answer': \"Navigate to Billing → Update payment method and enter your new details.\"})\n",
        "    \n",
        "    rows.append({'id': 'faq_4', 'type': 'faq', 'subject': 'Do you offer refunds?', \n",
        "                 'text': \"Refunds are available within 14 days of purchase if eligible.\", \n",
        "                 'answer': \"Yes, refunds are possible within 14 days. Contact support for help.\"})\n",
        "    \n",
        "    rows.append({'id': 'faq_5', 'type': 'faq', 'subject': 'How to contact support?', \n",
        "                 'text': \"You can contact support via email at support@example.com or through the Help Center chat.\", \n",
        "                 'answer': \"Email support@example.com or open the Help Center chat.\"})\n",
        "\n",
        "    # Past tickets\n",
        "    rows.append({'id': 'ticket_1001', 'type': 'ticket', 'subject': 'Unable to login after password reset', \n",
        "                 'text': \"I tried resetting my password but the link expired. I requested a new link twice and still can't login.\", \n",
        "                 'answer': 'Support provided a new link and advised to clear browser cache.'})\n",
        "    \n",
        "    rows.append({'id': 'ticket_1002', 'type': 'ticket', 'subject': 'Billing charged twice', \n",
        "                 'text': \"I was charged twice for last month. Please refund one charge.\", \n",
        "                 'answer': 'Refund issued and billing team notified.'})\n",
        "    \n",
        "    rows.append({'id': 'ticket_1003', 'type': 'ticket', 'subject': 'Unable to update payment method', \n",
        "                 'text': \"When I try to update my credit card info, it says invalid card even though it’s valid.\", \n",
        "                 'answer': 'Asked customer to try another browser and confirmed card was added manually by billing team.'})\n",
        "    \n",
        "    rows.append({'id': 'ticket_1004', 'type': 'ticket', 'subject': 'App keeps crashing', \n",
        "                 'text': \"The mobile app crashes every time I try to open the dashboard.\", \n",
        "                 'answer': 'Engineering team released a patch. Customer asked to update to the latest version.'})\n",
        "    \n",
        "    rows.append({'id': 'ticket_1005', 'type': 'ticket', 'subject': 'Refund request not processed', \n",
        "                 'text': \"I requested a refund last week but haven’t received confirmation.\", \n",
        "                 'answer': 'Support confirmed refund was issued and shared transaction ID.'})\n",
        "    \n",
        "    rows.append({'id': 'ticket_1006', 'type': 'ticket', 'subject': 'Unable to login after password reset', \n",
        "                 'text': \"I tried resetting my password but the link expired. I requested a new link twice and still can't login.\", \n",
        "                 'answer': 'Support provided a new link and advised to clear browser cache.'})\n",
        "    \n",
        "    rows.append({'id': 'ticket_1007', 'type': 'ticket', 'subject': 'Billing charged twice', \n",
        "                 'text': \"I was charged twice for last month. Please refund one charge.\", \n",
        "                 'answer': 'Refund issued and billing team notified.'})\n",
        "    \n",
        "    rows.append({'id': 'ticket_1008', 'type': 'ticket', 'subject': 'Unable to update payment method', \n",
        "                 'text': \"When I try to update my credit card info, it says invalid card even though it’s valid.\", \n",
        "                 'answer': 'Asked customer to try another browser and confirmed card was added manually by billing team.'})\n",
        "    \n",
        "    rows.append({'id': 'ticket_1009', 'type': 'ticket', 'subject': 'App keeps crashing', \n",
        "                 'text': \"The mobile app crashes every time I try to open the dashboard.\", \n",
        "                 'answer': 'Engineering team released a patch. Customer asked to update to the latest version.'})\n",
        "    \n",
        "    rows.append({'id': 'ticket_1010', 'type': 'ticket', 'subject': 'Refund request not processed', \n",
        "                 'text': \"I requested a refund last week but haven’t received confirmation.\", \n",
        "                 'answer': 'Support confirmed refund was issued and shared transaction ID.'})\n",
        "\n",
        "\n",
        "    df = pd.DataFrame(rows)\n",
        "    df.to_csv(path, index=False)\n",
        "    \n",
        "generate_example_dataset()\n",
        "print('Wrote enriched support CSV to', SUPPORT_CSV)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5e19ec0c",
      "metadata": {},
      "source": [
        "## 6) Full pipeline example: load -> chunk -> embed -> index\n",
        "Run the cell to perform the pipeline. Ensure `EURI_API_KEY` and `EURI_EMBED_URL` are set (or modify the embedding function to call another provider).\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "id": "2ceb7147",
      "metadata": {},
      "outputs": [],
      "source": [
        "def run_pipeline(csv_path=SUPPORT_CSV, max_chars=500, use_faiss=True):\n",
        "    df = pd.read_csv(csv_path)\n",
        "    # build records\n",
        "    records = []\n",
        "    for _, r in df.iterrows():\n",
        "        records.append({'id': r['id'], 'text': r['text'], 'meta': {'type': r['type'], 'subject': r.get('subject',''), 'answer': r.get('answer','')}})\n",
        "    chunks = flatten_chunks(records, max_chars=max_chars)\n",
        "    texts = [c['text'] for c in chunks]\n",
        "    print(f' -> {len(chunks)} chunks to embed')\n",
        "    emb = get_embeddings_euri(texts)\n",
        "    print('-> embeddings shape', emb.shape)\n",
        "    if use_faiss:\n",
        "        index = build_faiss_index(emb)\n",
        "        return {'index': index, 'chunks': chunks, 'embeddings': emb}\n",
        "    else:\n",
        "        # Example placeholder for Qdrant - not executed here\n",
        "        return {'index': None, 'chunks': chunks, 'embeddings': emb}\n",
        "\n",
        "# Note: Do not call run_pipeline() automatically in the notebook file here - call it interactively after setting your API key.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ecb36e3d",
      "metadata": {},
      "source": [
        "## 7) Retrieval + simple answer suggestion\n",
        "\n",
        "This function takes a user query, embeds it via Euri, searches FAISS, and returns the top matched chunk texts + original answer (if available).\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "id": "93f8d034",
      "metadata": {},
      "outputs": [],
      "source": [
        "def retrieve_suggestions(query: str, state: Dict, top_k: int = 5) -> List[Dict]:\n",
        "    \"\"\"state must contain 'index', 'chunks' and optionally 'embeddings'\"\"\"\n",
        "    if state.get('index') is None:\n",
        "        raise RuntimeError('Index missing. Build index first with run_pipeline()')\n",
        "    q_emb = get_embeddings_euri([query])[0]\n",
        "    D, I = search_faiss(state['index'], q_emb, top_k=top_k)\n",
        "    results = []\n",
        "    for score, idx in zip(D, I):\n",
        "        c = state['chunks'][int(idx)]\n",
        "        results.append({'score': float(score), 'chunk_id': c['chunk_id'], 'text': c['text'], 'meta': c['meta']})\n",
        "    return results\n",
        "\n",
        "def format_suggestion(results: List[Dict]) -> str:\n",
        "    out = []\n",
        "    for r in results:\n",
        "        meta = r.get('meta', {})\n",
        "        answer = meta.get('answer')\n",
        "        out.append(f\"- (score={r['score']:.3f}) {r['text']}\\n  suggested answer: {answer}\")\n",
        "    return '\\n\\n'.join(out)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8446ef11",
      "metadata": {},
      "source": [
        "## 8) Retrieval-based chatbot (simple orchestration)\n",
        "The notebook includes a simple orchestration: for each user question, retrieve top chunks and show suggested answers. You can later pipe these into an LLM prompt for improved finalization.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "id": "ea07eafc",
      "metadata": {},
      "outputs": [],
      "source": [
        "def chat_loop(state: Dict):\n",
        "    print('Retrieval-based chatbot. Type \"exit\" to stop.')\n",
        "    while True:\n",
        "        q = input('\\nUser question: ')\n",
        "        if q.strip().lower() in ('exit','quit'):\n",
        "            break\n",
        "        results = retrieve_suggestions(q, state, top_k=5)\n",
        "        print('\\nSuggestions:')\n",
        "        print(format_suggestion(results))\n",
        "        print('\\n---\\n')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9b0e62c7",
      "metadata": {},
      "source": [
        "## 9) Qdrant example (optional)\n",
        "If you'd rather use Qdrant, start a Qdrant server and use `qdrant-client` to upload vectors. The following is an example snippet (not executed in this notebook automatically):\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b8c91135",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "from qdrant_client import QdrantClient\n",
            "client = QdrantClient(url='http://localhost:6333')\n",
            "collection_name = 'support_faqs'\n",
            "client.recreate_collection(collection_name, vectors_config={\n",
            "    'size': embeddings.shape[1],\n",
            "    'distance': 'Cosine'\n",
            "})\n",
            "points = []\n",
            "for i, emb in enumerate(embeddings):\n",
            "    points.append({'id': i, 'vector': emb.tolist(), 'payload': chunks[i]['meta']})\n",
            "client.upsert(collection_name, points)\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Example Qdrant usage (commented)\n",
        "qdrant_snippet = '''\n",
        "from qdrant_client import QdrantClient\n",
        "client = QdrantClient(url='http://localhost:6333')\n",
        "collection_name = 'dou_support_faqs'\n",
        "client.recreate_collection(collection_name, vectors_config={\n",
        "    'size': embeddings.shape[1],\n",
        "    'distance': 'Cosine'\n",
        "})\n",
        "points = []\n",
        "for i, emb in enumerate(embeddings):\n",
        "    points.append({'id': i, 'vector': emb.tolist(), 'payload': chunks[i]['meta']})\n",
        "client.upsert(collection_name, points)\n",
        "'''\n",
        "print(qdrant_snippet)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 10) Where to go from here\n",
        "- Hook the retrieval results to an LLM (OpenAI, Anthropic, local) to *generate* the final suggested reply using retrieved context.\n",
        "- Add caching for embeddings and persisted FAISS indexes to avoid re-embedding.\n",
        "- Add relevance feedback and reranking (BM25 over retrieved docs, or a neural re-ranker).\n",
        "- Add metadata filters (e.g., only search in `type=ticket` or by product).\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "### Save details\n",
        "Notebook generated programmatically on 2025-08-21T09:48:38.233718 UTC.\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "vectordb",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.18"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
