{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "43de5956",
   "metadata": {},
   "source": [
    "\n",
    "# Multilingual Knowledge Base — pipeline + retrieval (FAISS / Qdrant)\n",
    "\n",
    "This notebook builds a small multilingual knowledge-base demo:\n",
    "- Stores English documents plus translations / parallel content in **French**, **Hindi/Hinglish**, and **Wolof**.\n",
    "- Demonstrates a pipeline: **load → clean → chunk → embed → store** (FAISS or Qdrant).\n",
    "- Shows how to call an **EURi** embedding API (example code included — set `EURI_API_KEY` env var).\n",
    "- Provides a retrieval function that accepts queries in Hindi/Wolof/French and retrieves English documents.\n",
    "- Includes an optional experiment: **dual-index vs multilingual embeddings**.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8754e6cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Install packages as needed.\n"
     ]
    }
   ],
   "source": [
    "# Install required packages (uncomment when needed)\n",
    "!pip install -q faiss-cpu sentence-transformers numpy pandas scikit-learn requests qdrant-client transformers\n",
    "print('Install packages as needed.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "cbd30f3c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wrote data/docs.json and data/queries.json; sample docs: 3\n"
     ]
    }
   ],
   "source": [
    "# Create a small multilingual dataset (English canonical docs + multilingual queries/variants)\n",
    "docs = [\n",
    "    {\n",
    "        \"id\": \"doc_1\",\n",
    "        \"lang\": \"en\",\n",
    "        \"title\": \"How to reset your password\",\n",
    "        \"text\": \"To reset your password, go to Settings > Account > Reset password. Follow the instructions sent to your email.\"\n",
    "    },\n",
    "    {\n",
    "        \"id\": \"doc_2\",\n",
    "        \"lang\": \"en\",\n",
    "        \"title\": \"Refund policy for electronics\",\n",
    "        \"text\": \"We accept returns within 30 days for electronics provided they are in original condition and packaging. Refunds processed within 10 business days.\"\n",
    "    },\n",
    "    {\n",
    "        \"id\": \"doc_3\",\n",
    "        \"lang\": \"en\",\n",
    "        \"title\": \"Shipping times and carriers\",\n",
    "        \"text\": \"Standard shipping takes 3-7 business days. You can select express shipping at checkout for faster delivery.\"\n",
    "    }\n",
    "]\n",
    "\n",
    "multilingual_queries = {\n",
    "    \"hi\": [\n",
    "        \"पासवर्ड कैसे रीसेट करें\",\n",
    "        \"mera password reset kaise karu\"\n",
    "    ],\n",
    "    \"fr\": [\n",
    "        \"Comment réinitialiser votre mot de passe\",\n",
    "        \"politique de remboursement pour l'électronique\"\n",
    "    ],\n",
    "    \"wo\": [\n",
    "        \"Nataal sa password\",\n",
    "        \"Naka la dañu def refund yi?\"\n",
    "    ]\n",
    "}\n",
    "\n",
    "import os, json\n",
    "os.makedirs('data', exist_ok=True)\n",
    "with open('data/docs.json', 'w', encoding='utf-8') as f:\n",
    "    json.dump(docs, f, ensure_ascii=False, indent=2)\n",
    "with open('data/queries.json', 'w', encoding='utf-8') as f:\n",
    "    json.dump(multilingual_queries, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "print('Wrote data/docs.json and data/queries.json; sample docs:', len(docs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6c745532",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created 3 chunks\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "777"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Basic cleaning and chunking utilities\n",
    "import re\n",
    "from typing import List\n",
    "def clean_text(s: str) -> str:\n",
    "    s = s.replace('\\n', ' ').strip()\n",
    "    s = re.sub(r'\\s+', ' ', s)\n",
    "    return s\n",
    "from typing import List\n",
    "import re\n",
    "\n",
    "def chunk_text(text: str, max_chars: int = 800, overlap: int = 200) -> List[str]:\n",
    "    \"\"\"\n",
    "    Split text into chunks of max_chars with sentence boundaries respected\n",
    "    and optional overlap between chunks.\n",
    "    \"\"\"\n",
    "    sentences = re.split(r'(?<=[.!?])\\s+', text)\n",
    "    chunks = []\n",
    "    current = ''\n",
    "\n",
    "    for sent in sentences:\n",
    "        if len(current) + len(sent) <= max_chars:\n",
    "            current = (current + ' ' + sent).strip()\n",
    "        else:\n",
    "            if current:\n",
    "                chunks.append(current)\n",
    "            # start next chunk with overlap from the end of the previous chunk\n",
    "            if overlap > 0 and len(current) > overlap:\n",
    "                current = current[-overlap:] + ' ' + sent\n",
    "            else:\n",
    "                current = sent\n",
    "\n",
    "    if current:\n",
    "        chunks.append(current)\n",
    "\n",
    "    return chunks\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "from pathlib import Path\n",
    "import json\n",
    "docs = json.load(open('data/docs.json', 'r', encoding='utf-8'))\n",
    "\n",
    "import json\n",
    "from pathlib import Path\n",
    "\n",
    "chunked = []\n",
    "for d in docs:\n",
    "    text = clean_text(d[\"text\"])\n",
    "    parts = chunk_text(text, max_chars=200, overlap=50)  # 👈 overlap enabled\n",
    "    for i, p in enumerate(parts, start=1):  # start numbering at 1\n",
    "        chunked.append({\n",
    "            \"doc_id\": d[\"id\"],\n",
    "            \"chunk_id\": f\"{d['id']}_c{i}\",\n",
    "            \"text\": p,\n",
    "            \"title\": d[\"title\"],\n",
    "            \"lang\": d[\"lang\"]\n",
    "        })\n",
    "\n",
    "print(f\"Created {len(chunked)} chunks\")\n",
    "\n",
    "Path(\"data\").mkdir(exist_ok=True)\n",
    "Path(\"data/chunks.json\").write_text(\n",
    "    json.dumps(chunked, ensure_ascii=False, indent=2),\n",
    "    encoding=\"utf-8\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "96b7189c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Embedding helpers (EURi example + local fallback)\n",
    "import os, json, requests\n",
    "import numpy as np\n",
    "\n",
    "EURi_API_KEY = os.getenv(\"EURI_API_KEY\") or os.getenv(\"EURi_API_KEY\") or os.getenv(\"EURI_APIKEY\")\n",
    "\n",
    "def embed_texts_euri(texts):\n",
    "    # Example embedding call to EURi embedding API.\n",
    "    # Replace ENDPOINT with the actual EURi endpoint and set EURI_API_KEY in env vars.\n",
    "    if not EURi_API_KEY:\n",
    "        raise RuntimeError(\"EURi API key not set. Set EURI_API_KEY env var to use embed_texts_euri().\")\n",
    "\n",
    "    ENDPOINT = \"https://api.euri.example/v1/embeddings\"  # <-- replace with actual\n",
    "    headers = {\"Authorization\": f\"Bearer {EURi_API_KEY}\", \"Content-Type\": \"application/json\"}\n",
    "    payload = {\"input\": texts}\n",
    "    resp = requests.post(ENDPOINT, headers=headers, json=payload, timeout=30)\n",
    "    resp.raise_for_status()\n",
    "    data = resp.json()\n",
    "    return data.get(\"embeddings\", [])\n",
    "\n",
    "def embed_texts_local(texts, model_name=\"sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2\"):\n",
    "    try:\n",
    "        from sentence_transformers import SentenceTransformer\n",
    "    except Exception as e:\n",
    "        raise RuntimeError(\"sentence-transformers not installed. Install it or set EURI_API_KEY.\") from e\n",
    "    model = SentenceTransformer(model_name)\n",
    "    embs = model.encode(texts, show_progress_bar=False, convert_to_numpy=True)\n",
    "    return embs.tolist()\n",
    "\n",
    "def embed_texts(texts, use_euri=False):\n",
    "    if use_euri and EURi_API_KEY:\n",
    "        return embed_texts_euri(texts)\n",
    "    else:\n",
    "        return embed_texts_local(texts)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0db579ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\papam\\anaconda3\\envs\\vectordb\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FAISS index built (entries): 3\n"
     ]
    }
   ],
   "source": [
    "# Build a FAISS index and search functions\n",
    "import numpy as np\n",
    "import faiss\n",
    "import json\n",
    "from pathlib import Path\n",
    "\n",
    "chunks = json.load(open('data/chunks.json', 'r', encoding='utf-8'))\n",
    "texts = [c['text'] for c in chunks]\n",
    "\n",
    "try:\n",
    "    embeddings = embed_texts(texts, use_euri=False)\n",
    "except Exception as e:\n",
    "    print(\"Embedding error (fallback to zeros):\", e)\n",
    "    embeddings = [[0.0]*384 for _ in texts]\n",
    "\n",
    "emb_matrix = np.array(embeddings).astype('float32')\n",
    "dim = emb_matrix.shape[1]\n",
    "\n",
    "index = faiss.IndexFlatIP(dim)\n",
    "faiss.normalize_L2(emb_matrix)\n",
    "index.add(emb_matrix)\n",
    "\n",
    "faiss.write_index(index, 'data/faiss_index.idx')\n",
    "Path('data/faiss_meta.json').write_text(json.dumps(chunks, ensure_ascii=False, indent=2), encoding='utf-8')\n",
    "\n",
    "print(\"FAISS index built (entries):\", index.ntotal)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "fd238953",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hindi query -> [{'doc_id': 'doc_1', 'title': 'How to reset your password', 'text': 'To reset your password, go to Settings > Account > Reset password. Follow the instructions sent to your email.', 'score': 0.717039167881012}, {'doc_id': 'doc_2', 'title': 'Refund policy for electronics', 'text': 'We accept returns within 30 days for electronics provided they are in original condition and packaging. Refunds processed within 10 business days.', 'score': 0.0169343501329422}, {'doc_id': 'doc_3', 'title': 'Shipping times and carriers', 'text': 'Standard shipping takes 3-7 business days. You can select express shipping at checkout for faster delivery.', 'score': -0.09550174325704575}]\n",
      "French query -> [{'doc_id': 'doc_1', 'title': 'How to reset your password', 'text': 'To reset your password, go to Settings > Account > Reset password. Follow the instructions sent to your email.', 'score': 0.6974995732307434}, {'doc_id': 'doc_2', 'title': 'Refund policy for electronics', 'text': 'We accept returns within 30 days for electronics provided they are in original condition and packaging. Refunds processed within 10 business days.', 'score': 0.18006882071495056}, {'doc_id': 'doc_3', 'title': 'Shipping times and carriers', 'text': 'Standard shipping takes 3-7 business days. You can select express shipping at checkout for faster delivery.', 'score': -0.057236697524785995}]\n",
      "Wolof query (approx) -> [{'doc_id': 'doc_2', 'title': 'Refund policy for electronics', 'text': 'We accept returns within 30 days for electronics provided they are in original condition and packaging. Refunds processed within 10 business days.', 'score': 0.2665976583957672}, {'doc_id': 'doc_1', 'title': 'How to reset your password', 'text': 'To reset your password, go to Settings > Account > Reset password. Follow the instructions sent to your email.', 'score': 0.17494083940982819}, {'doc_id': 'doc_3', 'title': 'Shipping times and carriers', 'text': 'Standard shipping takes 3-7 business days. You can select express shipping at checkout for faster delivery.', 'score': 0.0036647580564022064}]\n"
     ]
    }
   ],
   "source": [
    "# Retrieval: accept a multilingual query, embed, and fetch top-k English docs\n",
    "import numpy as np\n",
    "import faiss, json\n",
    "from collections import OrderedDict\n",
    "\n",
    "def normalize(vec):\n",
    "    v = np.array(vec, dtype='float32')\n",
    "    faiss.normalize_L2(v)\n",
    "    return v\n",
    "\n",
    "index = faiss.read_index('data/faiss_index.idx')\n",
    "meta = json.load(open('data/faiss_meta.json', 'r', encoding='utf-8'))\n",
    "\n",
    "def retrieve(query, top_k=3, use_euri=False):\n",
    "    q_emb = embed_texts([query], use_euri=use_euri)\n",
    "    qv = normalize(q_emb)[0]\n",
    "    D, I = index.search(np.array([qv]), top_k)\n",
    "    results = []\n",
    "    for dist, idx in zip(D[0], I[0]):\n",
    "        if idx < 0: continue\n",
    "        item = meta[idx]\n",
    "        results.append({\"chunk_id\": item[\"chunk_id\"], \"doc_id\": item[\"doc_id\"], \"text\": item[\"text\"], \"score\": float(dist)})\n",
    "    seen = OrderedDict()\n",
    "    for r in results:\n",
    "        seen.setdefault(r[\"doc_id\"], r)\n",
    "    docs = json.load(open('data/docs.json', 'r', encoding='utf-8'))\n",
    "    doc_map = {d['id']: d for d in docs}\n",
    "    enriched = []\n",
    "    for doc_id, r in seen.items():\n",
    "        doc = doc_map.get(doc_id, {})\n",
    "        enriched.append({\"doc_id\": doc_id, \"title\": doc.get(\"title\"), \"text\": doc.get(\"text\"), \"score\": r[\"score\"]})\n",
    "    return enriched\n",
    "\n",
    "print(\"Hindi query ->\", retrieve(\"पासवर्ड कैसे रीसेट करें\"))\n",
    "print(\"French query ->\", retrieve(\"Comment réinitialiser votre mot de passe\"))\n",
    "print(\"Wolof query (approx) ->\", retrieve(\"Naka la dañu def refund yi?\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0c8d3f8c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "use_euri=False -> accuracy: 1.0 [True, True, True]\n",
      "use_euri=True -> accuracy: 0.0 [False, False, False]\n"
     ]
    }
   ],
   "source": [
    "# Dual-index vs multilingual embeddings experiment (toy)\n",
    "import json\n",
    "queries = json.load(open('data/queries.json', 'r', encoding='utf-8'))\n",
    "gold = {\n",
    "    \"पासवर्ड कैसे रीसेट करें\": \"doc_1\",\n",
    "    \"Comment réinitialiser votre mot de passe\": \"doc_1\",\n",
    "    \"Naka la dañu def refund yi?\": \"doc_2\"\n",
    "}\n",
    "\n",
    "def top1_match(query, use_euri=False):\n",
    "    res = retrieve(query, top_k=1, use_euri=use_euri)\n",
    "    if not res: return False\n",
    "    return res[0][\"doc_id\"] == gold.get(query)\n",
    "\n",
    "queries_to_test = list(gold.keys())\n",
    "for use_euri in (False, True):\n",
    "    results = []\n",
    "    for q in queries_to_test:\n",
    "        try:\n",
    "            m = top1_match(q, use_euri=use_euri)\n",
    "        except Exception as e:\n",
    "            m = False\n",
    "        results.append(m)\n",
    "    print(f\"use_euri={use_euri} -> accuracy:\", sum(results)/len(results), results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "04fd91ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Notebook ready.\n"
     ]
    }
   ],
   "source": [
    "# End of notebook. See top cells for instructions on using EURi and Qdrant.\n",
    "print('Notebook ready.')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "vectordb",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
