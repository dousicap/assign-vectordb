{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "de8557aa",
   "metadata": {},
   "source": [
    "# Academic Research Helper avec overlap dans le chunking\n",
    "\n",
    "Objectif: ingérer des abstracts académiques synthétiques, les découper en morceaux avec overlap, les encoder (via EURI si une clé est fournie, fallback local sinon), les stocker dans Qdrant et permettre une recherche sémantique pour des requêtes comme « transformers in computer vision ». Le chunking inclut désormais un overlap entre chunks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89a8d1f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import requests\n",
    "from typing import List, Dict, Any, Tuple\n",
    "\n",
    "from qdrant_client import QdrantClient\n",
    "from qdrant_client.models import VectorParams, Distance\n",
    "\n",
    "# Configuration\n",
    "VECTOR_SIZE = 128\n",
    "CHUNK_MAX_CHARS = 600\n",
    "CHUNK_OVERLAP_CHARS = 100  # overlap between consecutive chunks (in characters)\n",
    "COLLECTION_NAME = \"academic_papers\"\n",
    "\n",
    "# Qdrant host/port (override via environment if needed)\n",
    "QDRANT_HOST = os.getenv(\"QDRANT_HOST\", \"localhost\")\n",
    "QDRANT_PORT = int(os.getenv(\"QDRANT_PORT\", \"6333\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34ca2ece",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_dataset(n: int = 6) -> List[Dict[str, Any]]:\n",
    "    \"\"\"Génère un petit jeu de données synthétiques de papiers (id, title, abstract).\"\"\"\n",
    "    base_papers = [\n",
    "        {\"id\": \"P1\", \"title\": \"Transformers in Computer Vision: A Survey\",\n",
    "         \"abstract\": \"The transformer architecture has emerged as a powerful model for sequence modeling. This paper surveys transformer-based models in computer vision, including ViT, DeiT, and data-efficient variants. We discuss architectures, training regimes, and evaluation benchmarks.\"},\n",
    "        {\"id\": \"P2\", \"title\": \"Vision Transformers for Image Recognition\",\n",
    "         \"abstract\": \"We examine Vision Transformers (ViT) architectures, patch embeddings, and how self-attention captures long-range dependencies in images. We compare with CNN-based baselines and discuss efficiency and scalability.\"},\n",
    "        {\"id\": \"P3\", \"title\": \"Self-Attention Mechanisms in Vision Tasks\",\n",
    "         \"abstract\": \"Self-attention modules and their variants are applied to object detection, segmentation, and action recognition. We analyze computational trade-offs and show improvements on common benchmarks.\"},\n",
    "        {\"id\": \"P4\", \"title\": \"Transformers in Object Detection\",\n",
    "         \"abstract\": \"Transformers extend detection pipelines with query-based decoding and cross-attention. This paper surveys DETR-like models and improvements such as Deformable DETR and query-based attention.\"},\n",
    "        {\"id\": \"P5\", \"title\": \"Efficient Transformers for Vision\",\n",
    "         \"abstract\": \"We discuss efficiency techniques in transformers for vision, including sparse attention, kernel-based methods, and distillation strategies to reduce compute and memory footprints.\"},\n",
    "        {\"id\": \"P6\", \"title\": \"Multimodal Transformers for Vision-Language\",\n",
    "         \"abstract\": \"Extending transformers to vision-language tasks, we review approaches like CLIP and ALIGN, focusing on alignment between text and image representations and zero-shot capabilities.\"},\n",
    "    ]\n",
    "    out = []\n",
    "    for i in range(n):\n",
    "        p = base_papers[i % len(base_papers)].copy()\n",
    "        p[\"id\"] = f\"{p['id']}_{i}\"\n",
    "        out.append(p)\n",
    "    return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfcdef27",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(text: str) -> str:\n",
    "    text = text.strip()\n",
    "    text = re.sub(r\"\\s+\", \" \", text)\n",
    "    return text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ab17c1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def chunk_text(text: str, max_chars: int = CHUNK_MAX_CHARS, overlap_chars: int = CHUNK_OVERLAP_CHARS) -> List[str]:\n",
    "    \"\"\"Découpe le texte en chunks avec overlap entre chunks.\n",
    "\n",
    "    Paramètres:\n",
    "      max_chars: longueur maximale d'un chunk en caractères.\n",
    "      overlap_chars: nombre de caractères qui se chevauchent entre chunks consécutifs.\n",
    "    \"\"\"\n",
    "    if max_chars <= 0:\n",
    "        return [text]\n",
    "    chunks: List[str] = []\n",
    "    if not text:\n",
    "        return chunks\n",
    "    i = 0\n",
    "    n = len(text)\n",
    "    while i < n:\n",
    "        end = min(i + max_chars, n)\n",
    "        chunk = text[i:end].strip()\n",
    "        if chunk:\n",
    "            chunks.append(chunk)\n",
    "        if end >= n:\n",
    "            break\n",
    "        # Avancer en laissant un overlap de chars entre les chunks\n",
    "        i = max(0, end - overlap_chars)\n",
    "    return chunks\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e75359a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_embeddings(text: str) -> List[float]:\n",
    "    \"\"\"\n",
    "    Embedding function: utilise l’API EURI si clé présente; sinon, fallback local déterministe.\n",
    "    \"\"\"\n",
    "    api_key = os.getenv(\"EURI_API_KEY\")\n",
    "    endpoint = os.getenv(\"EURI_EMBEDDING_ENDPOINT\", \"https://api.euri.ai/v1/embed\")\n",
    "\n",
    "    if api_key:\n",
    "        try:\n",
    "            headers = {\"Authorization\": f\"Bearer {api_key}\", \"Content-Type\": \"application/json\"}\n",
    "            payload = {\"text\": text}\n",
    "            resp = requests.post(endpoint, json=payload, headers=headers, timeout=60)\n",
    "            resp.raise_for_status()\n",
    "            data = resp.json()\n",
    "            embedding = None\n",
    "            if isinstance(data, dict):\n",
    "                embedding = data.get(\"embedding\") or data.get(\"vector\") or data.get(\"embeddings\")\n",
    "            if isinstance(embedding, list):\n",
    "                return embedding\n",
    "        except Exception as e:\n",
    "            print(f\"[EURI] Embedding API failed: {e}\")\n",
    "\n",
    "    # Fallback deterministe (128-dim)\n",
    "    dim = VECTOR_SIZE\n",
    "    vec = [0.0] * dim\n",
    "    for idx, ch in enumerate(text):\n",
    "        vec[idx % dim] += (ord(ch) / 255.0)\n",
    "    norm = sum(v * v for v in vec) ** 0.5\n",
    "    if norm > 0:\n",
    "        vec = [v / norm for v in vec]\n",
    "    return vec\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2584412e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ensure_collection(client: QdrantClient, name: str) -> None:\n",
    "    try:\n",
    "        client.recreate_collection(\n",
    "            collection_name=name,\n",
    "            vectors_config=VectorParams(size=VECTOR_SIZE, distance=Distance.COSINE)\n",
    "        )\n",
    "        print(f\"Collection '{name}' recreated.\")\n",
    "    except Exception:\n",
    "        print(f\"Collection '{name}' already exists or could not be recreated.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b058457b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_and_store_dataset(papers: List[Dict[str, Any]], client: QdrantClient, collection_name: str = COLLECTION_NAME) -> List[str]:\n",
    "    \"\"\"Ingest papers into Qdrant: chunk + embed + store. Retourne les IDs des points.\"\"\"\n",
    "    points: List[Dict[str, Any]] = []\n",
    "    point_ids: List[str] = []\n",
    "    for paper in papers:\n",
    "        abstract = paper.get(\"abstract\", \"\")\n",
    "        clean = clean_text(abstract)\n",
    "        chunks = chunk_text(clean, max_chars=CHUNK_MAX_CHARS, overlap_chars=CHUNK_OVERLAP_CHARS)\n",
    "        for cid, chunk in enumerate(chunks):\n",
    "            vec = generate_embeddings(chunk)\n",
    "            point_id = f\"{paper['id']}_{cid}\"\n",
    "            payload = {\n",
    "                \"paper_id\": paper[\"id\"],\n",
    "                \"title\": paper.get(\"title\"),\n",
    "                \"chunk_id\": cid,\n",
    "                \"text\": chunk\n",
    "            }\n",
    "            points.append({\n",
    "                \"id\": point_id,\n",
    "                \"vector\": vec,\n",
    "                \"payload\": payload\n",
    "            })\n",
    "            point_ids.append(point_id)\n",
    "    if not points:\n",
    "        return []\n",
    "    client.upsert(collection_name=collection_name, points=points)\n",
    "    return point_ids\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fc133d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _extract_payload_score(res: Any) -> Tuple[Dict[str, Any], float]:\n",
    "    payload = getattr(res, \"payload\", None)\n",
    "    score = getattr(res, \"score\", None)\n",
    "    if payload is None and isinstance(res, dict):\n",
    "        payload = res.get(\"payload\", {})\n",
    "        score = res.get(\"score\", 0.0)\n",
    "    return payload if payload is not None else {}, float(score if score is not None else 0.0)\n",
    "\n",
    "def retrieve_papers(query: str, client: QdrantClient, collection_name: str = COLLECTION_NAME, top_k: int = 5) -> List[Dict[str, Any]]:\n",
    "    vec = generate_embeddings(query)\n",
    "    results = client.search(collection_name=collection_name, query_vector=vec, top=top_k, with_payload=True)\n",
    "    hits: List[Dict[str, Any]] = []\n",
    "    for r in results:\n",
    "        payload, score = _extract_payload_score(r)\n",
    "        title = payload.get(\"title\")\n",
    "        paper_id = payload.get(\"paper_id\")\n",
    "        chunk_id = payload.get(\"chunk_id\")\n",
    "        text = payload.get(\"text\", \"\")\n",
    "        hits.append({\n",
    "            \"score\": score,\n",
    "            \"paper_id\": paper_id,\n",
    "            \"title\": title,\n",
    "            \"chunk_id\": chunk_id,\n",
    "            \"text_snippet\": text[:200] + (\"...\" if len(text) > 200 else \"\")\n",
    "        })\n",
    "    return hits\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e88c1802",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_demo():\n",
    "    # Init Qdrant client\n",
    "    client = QdrantClient(host=QDRANT_HOST, port=QDRANT_PORT)\n",
    "\n",
    "    # Ensure collection exists/established\n",
    "    ensure_collection(client, COLLECTION_NAME)\n",
    "\n",
    "    # 1) Generate dataset\n",
    "    papers = generate_dataset(n=6)\n",
    "\n",
    "    # 2) Build and store (chunk + embed + store)\n",
    "    print(\"Ingesting papers into Qdrant...\")\n",
    "    stored_ids = build_and_store_dataset(papers, client, COLLECTION_NAME)\n",
    "    print(f\"Stored {len(stored_ids)} chunks across {len(papers)} papers.\")\n",
    "\n",
    "    # 3) Semantic query example\n",
    "    query = \"transformers in computer vision\"\n",
    "    print(f\"\n",
    "Query: {query}\")\n",
    "    results = retrieve_papers(query, client, COLLECTION_NAME, top_k=5)\n",
    "    print(\"Top results:\")\n",
    "    for r in results:\n",
    "        print(f\"- Paper ID: {r['paper_id']}, Title: {r['title']}, Score: {r['score']:.4f}\")\n",
    "        print(f\"  Snippet: {r['text_snippet']}\n",
    "\")\n",
    "\n",
    "# Execute the demo when running this notebook cell\n",
    "run_demo()\n"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
